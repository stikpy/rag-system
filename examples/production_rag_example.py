"""
Exemple de production RAG avec Langchain
========================================

Cet exemple montre un syst√®me RAG complet en production utilisant :
- Les meilleures pratiques Langchain
- Configuration optimis√©e
- Monitoring et observabilit√©
- Gestion d'erreurs robuste
"""

import os
import sys
import time
import logging
from pathlib import Path
from typing import List, Dict, Any, Optional
import asyncio
from datetime import datetime

# Ajouter le r√©pertoire src au path
sys.path.append(str(Path(__file__).parent.parent / "src"))

# Imports Langchain
from langchain_core.documents import Document
from langchain_core.vectorstores import InMemoryVectorStore
from langchain_community.vectorstores import FAISS, Chroma
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_mistralai import MistralAIEmbeddings, MistralAI
from langchain_cohere import CohereEmbeddings, CohereRerank
from langchain.chains import RetrievalQA, ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory
from langchain.prompts import PromptTemplate
from langchain.retrievers import BM25Retriever, EnsembleRetriever

# Imports du syst√®me RAG
from rag.utils.config import config
from rag.utils.logging import setup_logging

# Configuration du logging
logger = setup_logging(level="INFO", log_file="rag_production.log")


class ProductionRAGSystem:
    """Syst√®me RAG de production avec Langchain"""
    
    def __init__(
        self,
        embedding_provider: str = "mistral",
        llm_provider: str = "mistral",
        vector_store_type: str = "faiss",
        use_reranking: bool = True,
        use_memory: bool = True
    ):
        """
        Initialise le syst√®me RAG de production
        
        Args:
            embedding_provider: Fournisseur d'embeddings
            llm_provider: Fournisseur LLM
            vector_store_type: Type de vector store
            use_reranking: Utiliser le reranking
            use_memory: Utiliser la m√©moire conversationnelle
        """
        self.embedding_provider = embedding_provider
        self.llm_provider = llm_provider
        self.vector_store_type = vector_store_type
        self.use_reranking = use_reranking
        self.use_memory = use_memory
        
        # M√©triques
        self.metrics = {
            "queries_processed": 0,
            "total_retrieval_time": 0,
            "total_generation_time": 0,
            "cache_hits": 0,
            "errors": 0
        }
        
        # Initialiser les composants
        self._init_embeddings()
        self._init_llm()
        self._init_vector_store()
        self._init_reranker()
        self._init_memory()
        self._init_chain()
        
        logger.info(f"Syst√®me RAG de production initialis√©: {self.llm_provider} + {self.embedding_provider}")
    
    def _init_embeddings(self):
        """Initialise les embeddings"""
        try:
            if self.embedding_provider == "mistral":
                self.embeddings = MistralAIEmbeddings(
                    model=config.mistral_embedding_model,
                    api_key=config.mistral_api_key
                )
            elif self.embedding_provider == "openai":
                self.embeddings = OpenAIEmbeddings(
                    model=config.openai_embedding_model,
                    api_key=config.openai_api_key
                )
            elif self.embedding_provider == "cohere":
                self.embeddings = CohereEmbeddings(
                    model="embed-english-v3.0",
                    api_key=config.cohere_api_key
                )
            else:
                raise ValueError(f"Fournisseur d'embeddings non support√©: {self.embedding_provider}")
            
            logger.info(f"Embeddings {self.embedding_provider} initialis√©s")
            
        except Exception as e:
            logger.error(f"Erreur lors de l'initialisation des embeddings: {str(e)}")
            raise
    
    def _init_llm(self):
        """Initialise le LLM"""
        try:
            if self.llm_provider == "mistral":
                self.llm = MistralAI(
                    model=config.mistral_generation_model,
                    temperature=config.temperature,
                    api_key=config.mistral_api_key
                )
            elif self.llm_provider == "openai":
                self.llm = ChatOpenAI(
                    model=config.openai_generation_model,
                    temperature=config.temperature,
                    api_key=config.openai_api_key
                )
            else:
                raise ValueError(f"Fournisseur LLM non support√©: {self.llm_provider}")
            
            logger.info(f"LLM {self.llm_provider} initialis√©")
            
        except Exception as e:
            logger.error(f"Erreur lors de l'initialisation du LLM: {str(e)}")
            raise
    
    def _init_vector_store(self):
        """Initialise le vector store"""
        try:
            if self.vector_store_type == "faiss":
                self.vector_store = None  # Sera cr√©√© lors de l'ajout de documents
            elif self.vector_store_type == "chroma":
                self.vector_store = None  # Sera cr√©√© lors de l'ajout de documents
            elif self.vector_store_type == "inmemory":
                self.vector_store = InMemoryVectorStore(embedding=self.embeddings)
            else:
                raise ValueError(f"Type de vector store non support√©: {self.vector_store_type}")
            
            logger.info(f"Vector store {self.vector_store_type} initialis√©")
            
        except Exception as e:
            logger.error(f"Erreur lors de l'initialisation du vector store: {str(e)}")
            raise
    
    def _init_reranker(self):
        """Initialise le reranker"""
        if self.use_reranking:
            try:
                self.reranker = CohereRerank(
                    model=config.cohere_rerank_model,
                    api_key=config.cohere_api_key,
                    top_n=config.rerank_top_k
                )
                logger.info("Reranker Cohere initialis√©")
            except Exception as e:
                logger.warning(f"Reranker non disponible: {str(e)}")
                self.reranker = None
        else:
            self.reranker = None
    
    def _init_memory(self):
        """Initialise la m√©moire conversationnelle"""
        if self.use_memory:
            self.memory = ConversationBufferMemory(
                memory_key="chat_history",
                return_messages=True,
                output_key="answer"
            )
            logger.info("M√©moire conversationnelle initialis√©e")
        else:
            self.memory = None
    
    def _init_chain(self):
        """Initialise la cha√Æne RAG"""
        if self.vector_store is None:
            logger.warning("Vector store non initialis√© - cha√Æne non cr√©√©e")
            self.chain = None
            return
        
        try:
            if self.memory:
                self.chain = ConversationalRetrievalChain.from_llm(
                    llm=self.llm,
                    retriever=self.vector_store.as_retriever(),
                    memory=self.memory,
                    return_source_documents=True
                )
            else:
                self.chain = RetrievalQA.from_chain_type(
                    llm=self.llm,
                    chain_type="stuff",
                    retriever=self.vector_store.as_retriever(),
                    return_source_documents=True
                )
            
            logger.info("Cha√Æne RAG initialis√©e")
            
        except Exception as e:
            logger.error(f"Erreur lors de l'initialisation de la cha√Æne: {str(e)}")
            self.chain = None
    
    def add_documents(self, documents: List[Document]) -> bool:
        """
        Ajoute des documents au syst√®me
        
        Args:
            documents: Liste des documents √† ajouter
            
        Returns:
            True si succ√®s, False sinon
        """
        try:
            if self.vector_store_type == "faiss":
                self.vector_store = FAISS.from_documents(documents, self.embeddings)
            elif self.vector_store_type == "chroma":
                self.vector_store = Chroma.from_documents(
                    documents=documents,
                    embedding=self.embeddings,
                    persist_directory="./chroma_db"
                )
            elif self.vector_store_type == "inmemory":
                self.vector_store.add_documents(documents)
            
            # R√©initialiser la cha√Æne avec le nouveau vector store
            self._init_chain()
            
            logger.info(f"{len(documents)} documents ajout√©s au syst√®me")
            return True
            
        except Exception as e:
            logger.error(f"Erreur lors de l'ajout des documents: {str(e)}")
            self.metrics["errors"] += 1
            return False
    
    def query(self, question: str, use_reranking: bool = None) -> Dict[str, Any]:
        """
        Pose une question au syst√®me RAG
        
        Args:
            question: Question √† poser
            use_reranking: Utiliser le reranking (optionnel)
            
        Returns:
            Dictionnaire avec la r√©ponse et m√©tadonn√©es
        """
        start_time = time.time()
        use_reranking = use_reranking if use_reranking is not None else self.use_reranking
        
        try:
            if not self.chain:
                raise ValueError("Cha√Æne RAG non initialis√©e")
            
            # R√©cup√©ration
            retrieval_start = time.time()
            if use_reranking and self.reranker:
                # R√©cup√©ration avec reranking
                retriever = self.vector_store.as_retriever(search_kwargs={"k": 10})
                documents = retriever.invoke(question)
                reranked_docs = self.reranker.compress_documents(documents, question)
                
                # Utiliser les documents rerank√©s
                context = "\n".join([doc.page_content for doc in reranked_docs])
                response = self._generate_with_context(question, context)
                source_documents = reranked_docs
            else:
                # R√©cup√©ration standard
                if self.memory:
                    result = self.chain({"question": question})
                else:
                    result = self.chain({"query": question})
                
                response = result["answer"]
                source_documents = result.get("source_documents", [])
            
            retrieval_time = time.time() - retrieval_start
            
            # G√©n√©ration
            generation_start = time.time()
            if not use_reranking or not self.reranker:
                # La g√©n√©ration a d√©j√† √©t√© faite dans la cha√Æne
                generation_time = 0
            else:
                generation_time = time.time() - generation_start
            
            total_time = time.time() - start_time
            
            # Mettre √† jour les m√©triques
            self.metrics["queries_processed"] += 1
            self.metrics["total_retrieval_time"] += retrieval_time
            self.metrics["total_generation_time"] += generation_time
            
            return {
                "answer": response,
                "source_documents": source_documents,
                "num_sources": len(source_documents),
                "retrieval_time": retrieval_time,
                "generation_time": generation_time,
                "total_time": total_time,
                "reranked": use_reranking and self.reranker is not None
            }
            
        except Exception as e:
            logger.error(f"Erreur lors de la requ√™te: {str(e)}")
            self.metrics["errors"] += 1
            return {
                "answer": f"Erreur lors du traitement: {str(e)}",
                "source_documents": [],
                "error": str(e)
            }
    
    def _generate_with_context(self, question: str, context: str) -> str:
        """G√©n√®re une r√©ponse avec un contexte donn√©"""
        prompt = PromptTemplate.from_template("""
        Contexte: {context}
        
        Question: {question}
        
        R√©ponse bas√©e sur le contexte:
        """)
        
        formatted_prompt = prompt.format(context=context, question=question)
        response = self.llm.invoke(formatted_prompt)
        return response.content
    
    def get_metrics(self) -> Dict[str, Any]:
        """Retourne les m√©triques du syst√®me"""
        if self.metrics["queries_processed"] > 0:
            avg_retrieval_time = self.metrics["total_retrieval_time"] / self.metrics["queries_processed"]
            avg_generation_time = self.metrics["total_generation_time"] / self.metrics["queries_processed"]
        else:
            avg_retrieval_time = 0
            avg_generation_time = 0
        
        return {
            "queries_processed": self.metrics["queries_processed"],
            "avg_retrieval_time": avg_retrieval_time,
            "avg_generation_time": avg_generation_time,
            "cache_hits": self.metrics["cache_hits"],
            "errors": self.metrics["errors"],
            "error_rate": self.metrics["errors"] / max(self.metrics["queries_processed"], 1)
        }
    
    def clear_memory(self):
        """Efface la m√©moire conversationnelle"""
        if self.memory:
            self.memory.clear()
            logger.info("M√©moire conversationnelle effac√©e")


def create_sample_documents() -> List[Document]:
    """Cr√©e des documents d'exemple"""
    documents = [
        Document(
            page_content="L'intelligence artificielle est une technologie qui permet aux machines d'apprendre et de prendre des d√©cisions de mani√®re autonome.",
            metadata={"source": "article_1", "category": "technology", "language": "fr"}
        ),
        Document(
            page_content="Le machine learning est une sous-discipline de l'IA qui se concentre sur l'apprentissage automatique √† partir de donn√©es.",
            metadata={"source": "article_2", "category": "technology", "language": "fr"}
        ),
        Document(
            page_content="Les r√©seaux de neurones artificiels sont inspir√©s du fonctionnement du cerveau humain et permettent de r√©soudre des probl√®mes complexes.",
            metadata={"source": "article_3", "category": "science", "language": "fr"}
        ),
        Document(
            page_content="L'IA g√©n√©rative comme ChatGPT peut cr√©er du contenu textuel de qualit√© et ouvre de nouvelles possibilit√©s cr√©atives.",
            metadata={"source": "article_4", "category": "applications", "language": "fr"}
        ),
        Document(
            page_content="L'√©thique de l'IA est un enjeu majeur qui n√©cessite une r√©flexion sur l'√©quit√©, la transparence et le respect de la vie priv√©e.",
            metadata={"source": "article_5", "category": "ethics", "language": "fr"}
        )
    ]
    return documents


def test_production_rag():
    """Test du syst√®me RAG de production"""
    
    print("üöÄ Test du syst√®me RAG de production...")
    
    # 1. Initialiser le syst√®me
    rag_system = ProductionRAGSystem(
        embedding_provider="mistral",
        llm_provider="mistral",
        vector_store_type="faiss",
        use_reranking=True,
        use_memory=True
    )
    
    # 2. Cr√©er et ajouter des documents
    documents = create_sample_documents()
    success = rag_system.add_documents(documents)
    
    if not success:
        print("‚ùå Erreur lors de l'ajout des documents")
        return
    
    print(f"‚úÖ {len(documents)} documents ajout√©s")
    
    # 3. Tester diff√©rentes requ√™tes
    test_queries = [
        "Qu'est-ce que l'intelligence artificielle ?",
        "Comment fonctionne le machine learning ?",
        "Expliquez les r√©seaux de neurones",
        "Quels sont les enjeux √©thiques de l'IA ?",
        "Qu'est-ce que l'IA g√©n√©rative ?"
    ]
    
    print("\nüîç Test des requ√™tes...")
    
    for i, query in enumerate(test_queries, 1):
        print(f"\nüìù Requ√™te {i}: {query}")
        
        start_time = time.time()
        result = rag_system.query(query, use_reranking=True)
        total_time = time.time() - start_time
        
        print(f"üí¨ R√©ponse: {result['answer']}")
        print(f"üìö Sources: {result['num_sources']} documents")
        print(f"‚è±Ô∏è  Temps: {total_time:.2f}s")
        print(f"üîÑ Reranking: {'Oui' if result.get('reranked', False) else 'Non'}")
        
        if 'error' in result:
            print(f"‚ùå Erreur: {result['error']}")
    
    # 4. Afficher les m√©triques
    print("\nüìä M√©triques du syst√®me:")
    metrics = rag_system.get_metrics()
    for key, value in metrics.items():
        if isinstance(value, float):
            print(f"  {key}: {value:.3f}")
        else:
            print(f"  {key}: {value}")
    
    # 5. Test de la m√©moire conversationnelle
    print("\nüí≠ Test de la m√©moire conversationnelle...")
    
    # Premi√®re question
    result1 = rag_system.query("Qu'est-ce que l'IA ?")
    print(f"R√©ponse 1: {result1['answer'][:100]}...")
    
    # Question de suivi
    result2 = rag_system.query("Peux-tu me donner plus de d√©tails ?")
    print(f"R√©ponse 2: {result2['answer'][:100]}...")
    
    # Effacer la m√©moire
    rag_system.clear_memory()
    print("üßπ M√©moire effac√©e")


def benchmark_performance():
    """Benchmark des performances"""
    
    print("\n‚ö° Benchmark des performances...")
    
    # Test avec diff√©rents fournisseurs
    providers = [
        ("mistral", "mistral"),
        ("openai", "openai"),
    ]
    
    for embedding_provider, llm_provider in providers:
        print(f"\nüîß Test avec {embedding_provider} + {llm_provider}...")
        
        try:
            # Initialiser le syst√®me
            rag_system = ProductionRAGSystem(
                embedding_provider=embedding_provider,
                llm_provider=llm_provider,
                vector_store_type="inmemory",
                use_reranking=False,
                use_memory=False
            )
            
            # Ajouter des documents
            documents = create_sample_documents()
            rag_system.add_documents(documents)
            
            # Test de performance
            query = "Qu'est-ce que l'intelligence artificielle ?"
            times = []
            
            for _ in range(5):  # 5 tests
                start_time = time.time()
                result = rag_system.query(query)
                end_time = time.time()
                times.append(end_time - start_time)
            
            avg_time = sum(times) / len(times)
            print(f"  Temps moyen: {avg_time:.2f}s")
            print(f"  R√©ponse: {result['answer'][:50]}...")
            
        except Exception as e:
            print(f"  ‚ùå Erreur: {str(e)}")


def main():
    """Fonction principale"""
    
    print("üéØ Syst√®me RAG de Production avec Langchain")
    print("=" * 50)
    
    try:
        # Test principal
        test_production_rag()
        
        # Benchmark
        benchmark_performance()
        
        print("\nüéâ Tests termin√©s avec succ√®s !")
        
    except Exception as e:
        print(f"\n‚ùå Erreur lors des tests: {str(e)}")
        logger.error(f"Erreur dans main: {str(e)}")


if __name__ == "__main__":
    main()
